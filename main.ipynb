{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impoert Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ViTConfig\n",
    "from collections import OrderedDict \n",
    "from typing import Optional, Tuple, Union\n",
    "from src.transformers.activations import ACT2FN\n",
    "from src.transformers.modeling_outputs import BaseModelOutput\n",
    "from src.transformers.models.vit.modeling_vit import ViTEmbeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN BackBones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelHook(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch module to retrieve the output of specified layers in a model using forward hooks.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model from which the output is to be retrieved.\n",
    "        output_layers (list): A list of layer names for which the output needs to be captured.\n",
    "\n",
    "    Attributes:\n",
    "        output_layers (list): A list of layer names for which the output needs to be captured.\n",
    "        selected_out (OrderedDict): A dictionary to store the output of selected layers.\n",
    "        model (nn.Module): The model from which the output is retrieved.\n",
    "        fhooks (list): A list to hold the forward hooks registered for selected layers.\n",
    "\n",
    "    Methods:\n",
    "        forward_hook(layer_name): Method to create a forward hook for a specific layer.\n",
    "        forward(x): Forward method of the module.\n",
    "\n",
    "    Returns:\n",
    "        out (torch.Tensor): The output tensor from the model's forward pass.\n",
    "        selected_out (OrderedDict): A dictionary containing the output tensors of selected layers.\n",
    "\n",
    "    Example:\n",
    "        # Instantiate a ResNet model\n",
    "        resnet_model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "        # Define layers for which output needs to be captured\n",
    "        output_layers = ['conv1', 'layer1', 'layer2']\n",
    "\n",
    "        # Instantiate ModelHook module\n",
    "        model_hook = ModelHook(resnet_model, output_layers)\n",
    "\n",
    "        # Forward pass\n",
    "        inputs = torch.randn(1, 3, 224, 224)\n",
    "        out, selected_out = model_hook(inputs)\n",
    "\n",
    "        # Output of selected layers can be accessed from 'selected_out' dictionary\n",
    "        print(selected_out)\n",
    "    \"\"\"\n",
    "    def __init__(self,model, output_layers, *args):\n",
    "        super().__init__(*args)\n",
    "        self.output_layers = output_layers\n",
    "        # print(self.output_layers)\n",
    "        self.selected_out = OrderedDict()\n",
    "        #PRETRAINED MODEL\n",
    "        self.model = model\n",
    "        self.fhooks = []\n",
    "\n",
    "        for l in list(self.model._modules.keys()):\n",
    "            if l in self.output_layers:\n",
    "                self.fhooks.append(getattr(self.model,l).register_forward_hook(self.forward_hook(l)))\n",
    "    \n",
    "    def forward_hook(self,layer_name):\n",
    "        def hook(module, input, output):\n",
    "            self.selected_out[layer_name] = output\n",
    "        return hook\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out, self.selected_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBackBone(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch module implementing a CNN backbone for feature extraction.\n",
    "\n",
    "    Args:\n",
    "        hidden_size (int): The length of embedded vector that will be mounted in ViT.\n",
    "        hidden_dropout_prob (float): The dropout probability for the hidden layer.\n",
    "        attention_probs_dropout_prob (float): The dropout probability for attention weights.\n",
    "\n",
    "    Attributes:\n",
    "        resnet50_module (nn.Module): ResNet50 module with pretrained weights as the CNN backbone.\n",
    "        avg_pool (nn.Module): Average pooling layer.\n",
    "        middle_linear (nn.Module): Linear layer to transform middle layer's output.\n",
    "        end_linear (nn.Module): Linear layer to transform end layer's output.\n",
    "        CNN_block (ModelHook): ModelHook instance to extract outputs from specific layers.\n",
    "\n",
    "    Methods:\n",
    "        forward(x): Forward method of the module.\n",
    "\n",
    "    Returns:\n",
    "        CNN_end_layer_out (torch.Tensor): The output tensor from the end layer of the CNN backbone.\n",
    "        CNN_middle_layer_out (torch.Tensor): The output tensor from the middle layer of the CNN backbone.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, hidden_size=768, hidden_dropout_prob=0.0, attention_probs_dropout_prob=0.0):\n",
    "        super().__init__(*args)\n",
    "        # Load and implement models\n",
    "        self.resnet50_module = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=4, stride=4)\n",
    "        self.middle_linear = nn.Linear(512, hidden_size)\n",
    "        self.end_linear = nn.Linear(2048, hidden_size)\n",
    "        \n",
    "        # Remove extra layers from CNN block (ResNetx) and add hook to it\n",
    "        layers_dict = {name: module for name,\n",
    "        module in zip(list(self.resnet50_module._modules.keys()),\n",
    "                             list(self.resnet50_module.children())[:-2])} #all layers except last two\n",
    "        self.resnet50_module = torch.nn.Sequential(OrderedDict(layers_dict))\n",
    "        self.CNN_block = ModelHook(self.resnet50_module, [\"layer2\", \"layer4\"])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the CNN backbone.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            CNN_end_layer_out (torch.Tensor): Output tensor from the end layer of the CNN backbone.\n",
    "            CNN_middle_layer_out (torch.Tensor): Output tensor from the middle layer of the CNN backbone.\n",
    "        \"\"\"\n",
    "        # Send original input through the ResNetx model to extract middle and end layer output\n",
    "        _, CNN_outputs = self.CNN_block(x)\n",
    "        \n",
    "        # Generate matrices of size (-1, 512, 16, 16) out of layer2 of ResNetx\n",
    "        CNN_middle_layer_out = self.avg_pool(CNN_outputs[\"layer2\"])\n",
    "        # Generate matrices of size (-1, 2048, 16, 16) out of layer4 of ResNetx\n",
    "        CNN_end_layer_out = CNN_outputs[\"layer4\"]\n",
    "        \n",
    "        # Merge dimensions of height and width of matrices and swap dimensions to generate 256 vectors with lengths 512 and 2048\n",
    "        CNN_middle_layer_out = CNN_middle_layer_out.permute(0, 2, 3, 1).contiguous().view(CNN_middle_layer_out.shape[0], -1, 512)\n",
    "        CNN_end_layer_out = CNN_end_layer_out.permute(0, 2, 3, 1).contiguous().view(CNN_middle_layer_out.shape[0], -1, 2048)\n",
    "        \n",
    "        # Send vectors through an MLP layer to generate vectors with length of 768\n",
    "        CNN_middle_layer_out = self.middle_linear(CNN_middle_layer_out)\n",
    "        CNN_end_layer_out = self.end_linear(CNN_end_layer_out)   \n",
    "\n",
    "        return CNN_end_layer_out, CNN_middle_layer_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViTEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTEmbeddings_modified(nn.Module):\n",
    "    \"\"\"\n",
    "    A modified version of ViT embedding layer for the CNN backbone. it's add CLS token and positional encoding to the input vector.\n",
    "\n",
    "    Args:\n",
    "        config (ViTConfig): Configuration settings for the ViT model.\n",
    "        num_patches (int): Number of patches in the input image.\n",
    "\n",
    "    Attributes:\n",
    "        cls_token (nn.Parameter): Learnable parameter representing the class token.\n",
    "        position_embeddings (nn.Parameter): Learnable parameter representing position embeddings.\n",
    "        dropout (nn.Dropout): Dropout layer.\n",
    "\n",
    "    Methods:\n",
    "        forward(pixel_values): Forward method of the module.\n",
    "\n",
    "    Returns:\n",
    "        embeddings (torch.Tensor): Output embeddings after processing the input pixel values.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, num_patches=16*16):\n",
    "        super().__init__()\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches + 1, config.hidden_size))\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self, pixel_values):\n",
    "        \"\"\"\n",
    "        Forward pass of the ViT embedding layer.\n",
    "\n",
    "        Args:\n",
    "            pixel_values (torch.Tensor): Input pixel values of the image.\n",
    "\n",
    "        Returns:\n",
    "            embeddings (torch.Tensor): Output embeddings after processing the input pixel values.\n",
    "        \"\"\"\n",
    "        batch_size = pixel_values.shape[0]\n",
    "        \n",
    "        embeddings = pixel_values\n",
    "        \n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n",
    "        embeddings = embeddings + self.position_embeddings\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViTSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" this class does not modified\"\"\"\n",
    "class ViTSelfOutput(nn.Module):\n",
    "    \"\"\"\n",
    "    The residual connection is defined in ViTLayer instead of here (as is the case with other models), due to the\n",
    "    layernorm applied before each block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTSelfAttention_modified(nn.Module):\n",
    "    \"\"\"\n",
    "    Modified self-attention mechanism for the ViT model.\n",
    "\n",
    "    Args:\n",
    "        config (ViTConfig): Configuration settings for the ViT model.\n",
    "\n",
    "    Attributes:\n",
    "        num_attention_heads (int): Number of attention heads.\n",
    "        attention_head_size (int): Size of each attention head.\n",
    "        all_head_size (int): Total size of all attention heads.\n",
    "        query (nn.Linear): Linear layer for query projection.\n",
    "        key (nn.Linear): Linear layer for key projection.\n",
    "        value (nn.Linear): Linear layer for value projection.\n",
    "        dropout (nn.Dropout): Dropout layer.\n",
    "\n",
    "    Methods:\n",
    "        transpose_for_scores(x): Reshape input tensor for attention scores calculation.\n",
    "        forward(K, Q, V, head_mask=None, output_attentions=False): Forward method of the module.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor]: Tuple containing the context layer and attention probabilities.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # NOTE: i think we should modify the following if\n",
    "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
    "            raise ValueError(\n",
    "                f\"The hidden size {config.hidden_size,} is not a multiple of the number of attention \"\n",
    "                f\"heads {config.num_attention_heads}.\"\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Reshape the input tensor for attention scores calculation.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Reshaped tensor.\n",
    "        \"\"\"\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self, K, Q, V, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass of the ViT self-attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            K (torch.Tensor): Key tensor.\n",
    "            Q (torch.Tensor): Query tensor.\n",
    "            V (torch.Tensor): Value tensor.\n",
    "            head_mask (Optional[torch.Tensor]): Optional tensor for masking heads.\n",
    "            output_attentions (bool): Whether to output attention probabilities.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor]: Tuple containing the context layer and attention probabilities.\n",
    "        \"\"\"\n",
    "        mixed_query_layer = self.query(Q)\n",
    "\n",
    "        key_layer = self.transpose_for_scores(self.key(K))\n",
    "        value_layer = self.transpose_for_scores(self.value(V))\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "        # Calculate attention scores\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "\n",
    "        # Apply softmax to obtain attention probabilities\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Apply head mask if provided\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        # Calculate context layer\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTSelfOutput(nn.Module):\n",
    "    '''this class does not get modified'''\n",
    "    \"\"\"\n",
    "    The residual connection is defined in ViTLayer instead of here (as is the case with other models), due to the\n",
    "    layernorm applied before each block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViTAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTAttention_modified(nn.Module):\n",
    "    \"\"\"\n",
    "    Modified attention layer for the ViT model.\n",
    "\n",
    "    Args:\n",
    "        config (ViTConfig): Configuration settings for the ViT model.\n",
    "\n",
    "    Attributes:\n",
    "        attention (ViTSelfAttention_modified): Self-attention mechanism.\n",
    "        output (ViTSelfOutput): Output layer.\n",
    "\n",
    "    Methods:\n",
    "        forward(K, Q, V, head_mask=None, output_attentions=False): Forward method of the module.\n",
    "\n",
    "    Returns:\n",
    "        Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]: Tuple containing output tensor and optionally attention probabilities.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.attention = ViTSelfAttention_modified(config)\n",
    "        self.output = ViTSelfOutput(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        K: torch.Tensor,\n",
    "        Q: torch.Tensor,\n",
    "        V: torch.Tensor,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Forward pass of the ViT attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            K (torch.Tensor): Key tensor.\n",
    "            Q (torch.Tensor): Query tensor.\n",
    "            V (torch.Tensor): Value tensor.\n",
    "            head_mask (Optional[torch.Tensor]): Optional tensor for masking heads.\n",
    "            output_attentions (bool): Whether to output attention probabilities.\n",
    "\n",
    "        Returns:\n",
    "            Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]: Tuple containing output tensor and optionally attention probabilities.\n",
    "        \"\"\"\n",
    "        self_outputs = self.attention(K, Q, V, head_mask, output_attentions)\n",
    "        \n",
    "        # Apply output layer\n",
    "        attention_output = self.output(self_outputs[0], None)\n",
    "\n",
    "        # Concatenate attention probabilities if needed\n",
    "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViTLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "'this class does not get modified'\n",
    "class ViTIntermediate(nn.Module):\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.intermediate_act_fn = config.hidden_act\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'this class does not get modified'\n",
    "class ViTOutput(nn.Module):\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        hidden_states = hidden_states #+ input_tensor\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTLayer_modified(nn.Module):\n",
    "    \"\"\"\n",
    "    Modified layer of the Vision Transformer model, corresponding to the Block class in the timm implementation.\n",
    "\n",
    "    Args:\n",
    "        config (ViTConfig): Configuration settings for the ViT model.\n",
    "\n",
    "    Attributes:\n",
    "        chunk_size_feed_forward (int): Chunk size for feed-forward operations.\n",
    "        seq_len_dim (int): Sequence length dimension.\n",
    "        attention (ViTAttention_modified): Modified self-attention mechanism.\n",
    "        intermediate_K (ViTIntermediate): Intermediate layer for keys.\n",
    "        intermediate_Q (ViTIntermediate): Intermediate layer for queries.\n",
    "        intermediate_V (ViTIntermediate): Intermediate layer for values.\n",
    "        output_K (ViTOutput): Output layer for keys.\n",
    "        output_Q (ViTOutput): Output layer for queries.\n",
    "        output_V (ViTOutput): Output layer for values.\n",
    "        layernorm_before_k (nn.LayerNorm): Layer normalization before keys.\n",
    "        layernorm_before_Q (nn.LayerNorm): Layer normalization before queries.\n",
    "        layernorm_before_V (nn.LayerNorm): Layer normalization before values.\n",
    "        layernorm_after_K (nn.LayerNorm): Layer normalization after keys.\n",
    "        layernorm_after_Q (nn.LayerNorm): Layer normalization after queries.\n",
    "        layernorm_after_V (nn.LayerNorm): Layer normalization after values.\n",
    "\n",
    "    Methods:\n",
    "        forward(K, Q, V, head_mask=None, output_attentions=False): Forward method of the module.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor]: Tuple containing modified output tensors for keys, queries, and values.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "        self.seq_len_dim = 1\n",
    "        self.attention = ViTAttention_modified(config)\n",
    "        \n",
    "        self.intermediate_K = ViTIntermediate(config)\n",
    "        self.intermediate_Q = ViTIntermediate(config)\n",
    "        self.intermediate_V = ViTIntermediate(config)\n",
    "        \n",
    "        self.output_K = ViTOutput(config)\n",
    "        self.output_Q = ViTOutput(config)\n",
    "        self.output_V = ViTOutput(config)\n",
    "        \n",
    "        self.layernorm_before_k = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.layernorm_before_Q = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.layernorm_before_V = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        \n",
    "        self.layernorm_after_K = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.layernorm_after_Q = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.layernorm_after_V = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        K: torch.Tensor,\n",
    "        Q: torch.Tensor,\n",
    "        V: torch.Tensor,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass of the ViT layer.\n",
    "\n",
    "        Args:\n",
    "            K (torch.Tensor): Key tensor.\n",
    "            Q (torch.Tensor): Query tensor.\n",
    "            V (torch.Tensor): Value tensor.\n",
    "            head_mask (Optional[torch.Tensor]): Optional tensor for masking heads.\n",
    "            output_attentions (bool): Whether to output attention probabilities.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor]: Tuple containing modified output tensors for keys, queries, and values.\n",
    "        \"\"\"\n",
    "        # layer norm\n",
    "        tk = self.layernorm_before_k(K)\n",
    "        tq = self.layernorm_before_Q(Q)\n",
    "        tv = self.layernorm_before_V(V)\n",
    "        \n",
    "        self_attention_outputs = self.attention(tk, tq, tv, head_mask, output_attentions=output_attentions)\n",
    "        attention_output = self_attention_outputs[0]\n",
    "        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "\n",
    "        # residual connection\n",
    "        K_prime = attention_output + K\n",
    "        Q_prime = attention_output + Q\n",
    "        V_prime = attention_output + V\n",
    "        # layer norm\n",
    "        layer_output_K = self.layernorm_after_K(K_prime)\n",
    "        layer_output_Q = self.layernorm_after_Q(Q_prime)\n",
    "        layer_output_V = self.layernorm_after_V(V_prime)\n",
    "        # Feed Forward layer\n",
    "        layer_output_K = self.intermediate_K(layer_output_K)\n",
    "        layer_output_Q = self.intermediate_Q(layer_output_Q)\n",
    "        layer_output_V = self.intermediate_V(layer_output_V)\n",
    "        # second residual connection is done here\n",
    "        layer_output_K = self.output_K(layer_output_K, None)\n",
    "        layer_output_Q = self.output_Q(layer_output_Q, None)\n",
    "        layer_output_V = self.output_V(layer_output_V, None)\n",
    "        \n",
    "        return (layer_output_K, layer_output_Q, layer_output_V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE in this model we dont use this block modify docstring for future use.\n",
    "\n",
    "# class ViTLayer_modified_last_layer(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Modified last layer of the Vision Transformer model, corresponding to the Block class in the timm implementation.\n",
    "\n",
    "#     Args:\n",
    "#         config (ViTConfig): Configuration settings for the ViT model.\n",
    "\n",
    "#     Attributes:\n",
    "#         chunk_size_feed_forward (int): Chunk size for feed-forward operations.\n",
    "#         seq_len_dim (int): Sequence length dimension.\n",
    "#         attention (ViTAttention_modified): Modified self-attention mechanism.\n",
    "#         intermediate (ViTIntermediate): Intermediate layer.\n",
    "#         output (ViTOutput): Output layer.\n",
    "#         layernorm_before_k (nn.LayerNorm): Layer normalization before keys.\n",
    "#         layernorm_before_Q (nn.LayerNorm): Layer normalization before queries.\n",
    "#         layernorm_before_V (nn.LayerNorm): Layer normalization before values.\n",
    "#         layernorm_after (nn.LayerNorm): Layer normalization after self-attention.\n",
    "\n",
    "#     Methods:\n",
    "#         forward(K, Q, V, head_mask=None, output_attentions=False): Forward method of the module.\n",
    "\n",
    "#     Returns:\n",
    "#         torch.Tensor: Output tensor after processing through the layer.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, config: ViTConfig) -> None:\n",
    "#         super().__init__()\n",
    "#         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "#         self.seq_len_dim = 1\n",
    "#         self.attention = ViTAttention_modified(config)\n",
    "#         self.intermediate = ViTIntermediate(config)\n",
    "#         self.output = ViTOutput(config)\n",
    "#         self.layernorm_before_k = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "#         self.layernorm_before_Q = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "#         self.layernorm_before_V = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "#         self.layernorm_after = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         K: torch.Tensor,\n",
    "#         Q: torch.Tensor,\n",
    "#         V: torch.Tensor,\n",
    "#         head_mask: Optional[torch.Tensor] = None,\n",
    "#         output_attentions: bool = False,\n",
    "#     ) -> torch.Tensor:\n",
    "#         \"\"\"\n",
    "#         Forward pass of the ViT last layer.\n",
    "\n",
    "#         Args:\n",
    "#             K (torch.Tensor): Key tensor.\n",
    "#             Q (torch.Tensor): Query tensor.\n",
    "#             V (torch.Tensor): Value tensor.\n",
    "#             head_mask (Optional[torch.Tensor]): Optional tensor for masking heads.\n",
    "#             output_attentions (bool): Whether to output attention probabilities.\n",
    "\n",
    "#         Returns:\n",
    "#             torch.Tensor: Output tensor after processing through the layer.\n",
    "#         \"\"\"\n",
    "#         tk = self.layernorm_before_k(K)\n",
    "#         tq = self.layernorm_before_Q(Q)\n",
    "#         tv = self.layernorm_before_V(V)\n",
    "#         self_attention_outputs = self.attention(tk, tq, tv, head_mask, output_attentions=output_attentions)\n",
    "#         attention_output = self_attention_outputs[0]\n",
    "#         outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "\n",
    "#         # first residual connection\n",
    "#         hidden_states = attention_output + K + Q + V\n",
    "       \n",
    "#         # in ViT, layernorm is also applied after self-attention\n",
    "#         layer_output = self.layernorm_after(hidden_states)\n",
    "#         layer_output = self.intermediate(layer_output)\n",
    "\n",
    "#         # second residual connection is done here\n",
    "#         layer_output = self.output(layer_output, hidden_states)\n",
    "\n",
    "#         return layer_output\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViTEncoder  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTEncoder_modified(nn.Module):\n",
    "    \"\"\"\n",
    "    Modified encoder layer of the Vision Transformer model.\n",
    "\n",
    "    Args:\n",
    "        config (ViTConfig): Configuration settings for the ViT model.\n",
    "\n",
    "    Attributes:\n",
    "        config (ViTConfig): Configuration settings for the ViT model.\n",
    "        layer (nn.ModuleList): List of modified ViT layers.\n",
    "        gradient_checkpointing (bool): Whether to use gradient checkpointing during training.\n",
    "\n",
    "    Methods:\n",
    "        forward(K, Q, V, head_mask=None, output_attentions=False, output_hidden_states=False, return_dict=True):\n",
    "            Forward method of the module.\n",
    "\n",
    "    Returns:\n",
    "        Union[tuple, BaseModelOutput]: Tuple containing output tensors if return_dict=True, otherwise a tuple\n",
    "        of tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        \n",
    "        self.layer = nn.ModuleList([ViTLayer_modified(config) for _ in range(config.num_hidden_layers)])\n",
    "        # self.last_layer = ViTLayer_modified_last_layer(config)\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        K: torch.Tensor,\n",
    "        Q: torch.Tensor,\n",
    "        V: torch.Tensor,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "        output_hidden_states: bool = False,\n",
    "        return_dict: bool = True,\n",
    "    ) -> Union[tuple, BaseModelOutput]:\n",
    "        \"\"\"\n",
    "        Forward pass of the ViT encoder.\n",
    "\n",
    "        Args:\n",
    "            K (torch.Tensor): Key tensor.\n",
    "            Q (torch.Tensor): Query tensor.\n",
    "            V (torch.Tensor): Value tensor.\n",
    "            head_mask (Optional[torch.Tensor]): Optional tensor for masking heads.\n",
    "            output_attentions (bool): Whether to output attention probabilities.\n",
    "            output_hidden_states (bool): Whether to output hidden states.\n",
    "            return_dict (bool): Whether to return output as a dictionary.\n",
    "\n",
    "        Returns:\n",
    "            Union[tuple, BaseModelOutput]: Tuple containing output tensors if return_dict=True, otherwise a tuple\n",
    "            of tensors.\n",
    "        \"\"\"\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        \n",
    "        # original_K = deepcopy(K)\n",
    "        # original_Q = deepcopy(Q)\n",
    "        # original_V = deepcopy(V)\n",
    "        \n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                \n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                layer_outputs = self._gradient_checkpointing_func(\n",
    "                    layer_module.__call__,\n",
    "                    K,\n",
    "                    Q,\n",
    "                    V,\n",
    "                    layer_head_mask,\n",
    "                    output_attentions,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(K, Q, V, layer_head_mask, output_attentions)\n",
    "\n",
    "            K = layer_outputs[0]\n",
    "            Q = layer_outputs[1]\n",
    "            V = layer_outputs[2]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "        \n",
    "        # output = self.last_layer(K, Q, V, layer_head_mask, output_attentions)\n",
    "        \n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n",
    "        \n",
    "        return (K,Q,V)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.1233, -0.3195, -0.2650,  ...,  0.3720,  0.2801, -0.0392],\n",
       "          [ 0.1248, -0.3193, -0.2646,  ...,  0.3707,  0.2801, -0.0387],\n",
       "          [ 0.1240, -0.3201, -0.2644,  ...,  0.3717,  0.2798, -0.0378],\n",
       "          ...,\n",
       "          [ 0.1238, -0.3200, -0.2652,  ...,  0.3720,  0.2795, -0.0380],\n",
       "          [ 0.1240, -0.3199, -0.2642,  ...,  0.3730,  0.2796, -0.0381],\n",
       "          [ 0.1245, -0.3207, -0.2654,  ...,  0.3715,  0.2799, -0.0389]],\n",
       " \n",
       "         [[ 0.1191, -0.3061, -0.2093,  ...,  0.3732,  0.2950, -0.0821],\n",
       "          [ 0.1198, -0.3053, -0.2093,  ...,  0.3715,  0.2948, -0.0816],\n",
       "          [ 0.1198, -0.3061, -0.2100,  ...,  0.3721,  0.2947, -0.0813],\n",
       "          ...,\n",
       "          [ 0.1195, -0.3067, -0.2102,  ...,  0.3727,  0.2948, -0.0806],\n",
       "          [ 0.1212, -0.3056, -0.2096,  ...,  0.3730,  0.2953, -0.0816],\n",
       "          [ 0.1204, -0.3073, -0.2100,  ...,  0.3723,  0.2955, -0.0815]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[-0.1344,  0.1289,  0.1145, -0.0674],\n",
       "         [-0.1300,  0.1417,  0.1298, -0.0713]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ViTIntegrated(nn.Module):\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.CNN_backbone = CNNBackBone(hidden_size = config.hidden_size)\n",
    "        self.ViT = ViTEncoder_modified(config)\n",
    "        # NOTE: the following code is designed for image size of 512*512\n",
    "        self.ViT_embedder_K = ViTEmbeddings_modified(config)\n",
    "        self.ViT_embedder_Q = ViTEmbeddings_modified(config)\n",
    "        self.ViT_embedder_V = ViTEmbeddings(config)\n",
    "        \n",
    "        # Classifier head\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels) if config.num_labels > 0 else nn.Identity()\n",
    "        \n",
    "    def forward(self, image):\n",
    "        if image.shape[-3:] != torch.Size([3,512,512]):\n",
    "            raise ValueError(f\"Input image dimension is not (3,512,512).\")\n",
    "        else:\n",
    "            K, Q = self.CNN_backbone(image)\n",
    "            K = self.ViT_embedder_K(K)\n",
    "            Q = self.ViT_embedder_Q(Q)\n",
    "            V = self.ViT_embedder_V(image)\n",
    "            if (K.shape != V.shape) or (K.shape != Q.shape):\n",
    "                raise ValueError(f\"Key, Quary, or Value dimension is not the same\")\n",
    "\n",
    "            ViT_outputs = self.ViT(\n",
    "                K,\n",
    "                Q,\n",
    "                V,\n",
    "            )\n",
    "\n",
    "            sequence_output = ViT_outputs[0] + ViT_outputs[1] + ViT_outputs[2]\n",
    "\n",
    "            logits = self.classifier(sequence_output[:, 0, :])\n",
    "            return sequence_output, logits\n",
    "            \n",
    "m = ViTIntegrated(ViTConfig(image_size =512,num_labels = 4,patch_size = 32)).to(\"cpu\")\n",
    "m(torch.randn((2,3,512,512)).to(\"cpu\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
