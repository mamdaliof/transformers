{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impoert Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ViTConfig\n",
    "from collections import OrderedDict \n",
    "from typing import Optional, Tuple, Union\n",
    "from src.transformers.activations import ACT2FN\n",
    "from src.transformers.modeling_outputs import BaseModelOutput\n",
    "from src.transformers.models.vit.modeling_vit import ViTEmbeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN BackBones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelHook(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch module to retrieve the output of specified layers in a model using forward hooks.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model from which the output is to be retrieved.\n",
    "        output_layers (list): A list of layer names for which the output needs to be captured.\n",
    "\n",
    "    Attributes:\n",
    "        output_layers (list): A list of layer names for which the output needs to be captured.\n",
    "        selected_out (OrderedDict): A dictionary to store the output of selected layers.\n",
    "        model (nn.Module): The model from which the output is retrieved.\n",
    "        fhooks (list): A list to hold the forward hooks registered for selected layers.\n",
    "\n",
    "    Methods:\n",
    "        forward_hook(layer_name): Method to create a forward hook for a specific layer.\n",
    "        forward(x): Forward method of the module.\n",
    "\n",
    "    Returns:\n",
    "        out (torch.Tensor): The output tensor from the model's forward pass.\n",
    "        selected_out (OrderedDict): A dictionary containing the output tensors of selected layers.\n",
    "\n",
    "    Example:\n",
    "        # Instantiate a ResNet model\n",
    "        resnet_model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "        # Define layers for which output needs to be captured\n",
    "        output_layers = ['conv1', 'layer1', 'layer2']\n",
    "\n",
    "        # Instantiate ModelHook module\n",
    "        model_hook = ModelHook(resnet_model, output_layers)\n",
    "\n",
    "        # Forward pass\n",
    "        inputs = torch.randn(1, 3, 224, 224)\n",
    "        out, selected_out = model_hook(inputs)\n",
    "\n",
    "        # Output of selected layers can be accessed from 'selected_out' dictionary\n",
    "        print(selected_out)\n",
    "    \"\"\"\n",
    "    def __init__(self,model, output_layers, *args):\n",
    "        super().__init__(*args)\n",
    "        self.output_layers = output_layers\n",
    "        # print(self.output_layers)\n",
    "        self.selected_out = OrderedDict()\n",
    "        #PRETRAINED MODEL\n",
    "        self.model = model\n",
    "        self.fhooks = []\n",
    "\n",
    "        for l in list(self.model._modules.keys()):\n",
    "            if l in self.output_layers:\n",
    "                self.fhooks.append(getattr(self.model,l).register_forward_hook(self.forward_hook(l)))\n",
    "    \n",
    "    def forward_hook(self,layer_name):\n",
    "        def hook(module, input, output):\n",
    "            self.selected_out[layer_name] = output\n",
    "        return hook\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out, self.selected_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBackBone(nn.Module):\n",
    "    def __init__(self, *args, hidden_size = 768, hidden_dropout_prob = 0.0,attention_probs_dropout_prob = 0.0):\n",
    "        super().__init__(*args)\n",
    "        # Load and impliment models\n",
    "        self.resnet50_module = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2).to(\"cuda\")\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=4, stride=4).to(\"cuda\")\n",
    "        self.middle_linear = nn.Linear(512, hidden_size).to(\"cuda\")\n",
    "        self.end_linear = nn.Linear(2048, hidden_size).to(\"cuda\")\n",
    "        \n",
    "        # Remove extra layers from CNN block (ResNetx) and add hook to it\n",
    "        layers_dict = {name: module for name,\n",
    "        module in zip(list(self.resnet50_module._modules.keys()),\n",
    "                             list(self.resnet50_module.children())[:-2])} #all layers except last two\n",
    "        self.resnet50_module = torch.nn.Sequential(OrderedDict(layers_dict))\n",
    "        self.CNN_block = ModelHook(self.resnet50_module, [\"layer2\",\"layer4\"])\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Send originad tgrough the Resnetx model to extract middle and end layer output\n",
    "        _, CNN_outputs = self.CNN_block(x)\n",
    "        \n",
    "        # Generate matrixe of size (-1,512,16,16) out of layer2 of ResNetx\n",
    "        CNN_middle_layer_out = self.avg_pool(CNN_outputs[\"layer2\"])\n",
    "        # Generate matrixe of size (-1,2048,16,16) out of layer4 of ResNetx\n",
    "        CNN_end_layer_out = CNN_outputs[\"layer4\"]\n",
    "        # print(CNN_middle_layer_out.shape)\n",
    "        # print(CNN_end_layer_out.shape)\n",
    "        \n",
    "        # Merge dimentions of heigth and width of matrixes into each other and swap dimentions to generate 256 vectors with the length of 512 and 2048\n",
    "        CNN_middle_layer_out = CNN_middle_layer_out.permute(0, 2, 3, 1).contiguous().view(CNN_middle_layer_out.shape[0], -1, 512)\n",
    "        CNN_end_layer_out = CNN_end_layer_out.permute(0, 2, 3, 1).contiguous().view(CNN_middle_layer_out.shape[0], -1, 2048)\n",
    "        # print(CNN_middle_layer_out.shape)\n",
    "        # print(CNN_end_layer_out.shape)\n",
    "        \n",
    "        # Send vectors throgh an MLP layer to make the generate vectors with length of 768\n",
    "        CNN_middle_layer_out = self.middle_linear(CNN_middle_layer_out)\n",
    "        CNN_end_layer_out = self.end_linear(CNN_end_layer_out)   \n",
    "        \n",
    "\n",
    "        return CNN_end_layer_out, CNN_middle_layer_out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViTEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTEmbeddings_modified(nn.Module):\n",
    "\n",
    "    def __init__(self, config, num_patches=16*16):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
    "        \n",
    "        # NOTE: THE ORIGINAL PatchEmbeddings ELIMINATED\n",
    "        # self.patch_embeddings = PatchEmbeddings(\n",
    "        #     image_size=config.image_size,\n",
    "        #     patch_size=config.patch_size,\n",
    "        #     num_channels=config.num_channels,\n",
    "        #     embed_dim=config.hidden_size,\n",
    "        # )\n",
    "        # num_patches = self.patch_embeddings.num_patches\n",
    "        # self.patch_embeddings = CNNBackBone()\n",
    "        \n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches + 1, config.hidden_size))\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self, pixel_values):\n",
    "        batch_size = pixel_values.shape[0]\n",
    "        \n",
    "        # NOTE: THE ORIGINAL embedings ELIMINATED\n",
    "        # embeddings = self.patch_embeddings(pixel_values)\n",
    "        embeddings = pixel_values\n",
    "        \n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n",
    "        embeddings = embeddings + self.position_embeddings\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViTSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" this class does not modified\"\"\"\n",
    "class ViTSelfOutput(nn.Module):\n",
    "    \"\"\"\n",
    "    The residual connection is defined in ViTLayer instead of here (as is the case with other models), due to the\n",
    "    layernorm applied before each block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTSelfAttention_modified(nn.Module):\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # NOTE: i think we should modify the followint if\n",
    "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
    "            raise ValueError(\n",
    "                f\"The hidden size {config.hidden_size,} is not a multiple of the number of attention \"\n",
    "                f\"heads {config.num_attention_heads}.\"\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self, K,Q,V, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n",
    "    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n",
    "        mixed_query_layer = self.query(Q)\n",
    "\n",
    "        key_layer = self.transpose_for_scores(self.key(K))\n",
    "        value_layer = self.transpose_for_scores(self.value(V))\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "\n",
    "        return outputs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTSelfOutput(nn.Module):\n",
    "    '''this class does not get modified'''\n",
    "    \"\"\"\n",
    "    The residual connection is defined in ViTLayer instead of here (as is the case with other models), due to the\n",
    "    layernorm applied before each block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViTAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTAttention_modified(nn.Module):\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.attention = ViTSelfAttention_modified(config)\n",
    "        self.output = ViTSelfOutput(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        K: torch.Tensor,\n",
    "        Q: torch.Tensor,\n",
    "        V: torch.Tensor,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n",
    "        self_outputs = self.attention(K,Q,V, head_mask, output_attentions)\n",
    "        # NOTE: K, Q, and V can have seperate dense layers\n",
    "        attention_output = self.output(self_outputs[0], None)\n",
    "\n",
    "        #NOTE: if output_attentions is True modify the following code\n",
    "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViTLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'this class does not get modified'\n",
    "class ViTIntermediate(nn.Module):\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.intermediate_act_fn = config.hidden_act\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'this class does not get modified'\n",
    "class ViTOutput(nn.Module):\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        hidden_states = hidden_states #+ input_tensor\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTLayer_modified(nn.Module):\n",
    "    \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n",
    "\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "        self.seq_len_dim = 1\n",
    "        self.attention = ViTAttention_modified(config)\n",
    "        \n",
    "        self.intermediate_K = ViTIntermediate(config)\n",
    "        self.intermediate_Q = ViTIntermediate(config)\n",
    "        self.intermediate_V = ViTIntermediate(config)\n",
    "        \n",
    "        self.output_K = ViTOutput(config)\n",
    "        self.output_Q = ViTOutput(config)\n",
    "        self.output_V = ViTOutput(config)\n",
    "        \n",
    "        self.layernorm_before_k = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.layernorm_before_Q = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.layernorm_before_V = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        \n",
    "        self.layernorm_after_K = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.layernorm_after_Q = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.layernorm_after_V = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        K: torch.Tensor,\n",
    "        Q: torch.Tensor,\n",
    "        V: torch.Tensor,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n",
    "        tk = self.layernorm_before_k(K)\n",
    "        tq = self.layernorm_before_Q(Q)\n",
    "        tv = self.layernorm_before_V(V)\n",
    "        self_attention_outputs = self.attention(tk, tq, tv, head_mask, output_attentions=output_attentions)\n",
    "        attention_output = self_attention_outputs[0]\n",
    "        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "\n",
    "        # first residual connection\n",
    "        K_prime = attention_output + K\n",
    "        Q_prime = attention_output + Q\n",
    "        V_prime = attention_output + V\n",
    "        # in ViT, layernorm is also applied after self-attention\n",
    "        layer_output_K = self.layernorm_after_K(K_prime)\n",
    "        layer_output_Q = self.layernorm_after_Q(Q_prime)\n",
    "        layer_output_V = self.layernorm_after_V(V_prime)\n",
    "        \n",
    "        layer_output_K = self.intermediate_K(layer_output_K)\n",
    "        layer_output_Q = self.intermediate_Q(layer_output_Q)\n",
    "        layer_output_V = self.intermediate_V(layer_output_V)\n",
    "        # second residual connection is done here\n",
    "        layer_output_K = self.output_K(layer_output_K, None)\n",
    "        layer_output_Q = self.output_Q(layer_output_Q, None)\n",
    "        layer_output_V = self.output_V(layer_output_V, None)\n",
    "        \n",
    "        # outputs = (layer_output,) + outputs\n",
    "\n",
    "        return (layer_output_K, layer_output_Q, layer_output_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTLayer_modified_last_layer(nn.Module):\n",
    "    \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n",
    "\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "        self.seq_len_dim = 1\n",
    "        self.attention = ViTAttention_modified(config)\n",
    "        self.intermediate = ViTIntermediate(config)\n",
    "        self.output = ViTOutput(config)\n",
    "        self.layernorm_before_k = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.layernorm_before_Q = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.layernorm_before_V = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.layernorm_after = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        K: torch.Tensor,\n",
    "        Q: torch.Tensor,\n",
    "        V: torch.Tensor,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n",
    "        tk = self.layernorm_before_k(K)\n",
    "        tq = self.layernorm_before_Q(Q)\n",
    "        tv = self.layernorm_before_V(V)\n",
    "        self_attention_outputs = self.attention(tk, tq, tv, head_mask, output_attentions=output_attentions)\n",
    "        attention_output = self_attention_outputs[0]\n",
    "        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "\n",
    "        # first residual connection\n",
    "        hidden_states = attention_output + K + Q + V\n",
    "       \n",
    "        # in ViT, layernorm is also applied after self-attention\n",
    "        layer_output = self.layernorm_after(hidden_states)\n",
    "        layer_output = self.intermediate(layer_output)\n",
    "\n",
    "        # second residual connection is done here\n",
    "        layer_output = self.output(layer_output, hidden_states)\n",
    "\n",
    "        # outputs = (layer_output,) + outputs\n",
    "\n",
    "        return layer_output\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViTEncoder  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTEncoder_modified(nn.Module):\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        \n",
    "        self.layer = nn.ModuleList([ViTLayer_modified(config) for _ in range(config.num_hidden_layers)])\n",
    "        # self.last_layer = ViTLayer_modified_last_layer(config)\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        K: torch.Tensor,\n",
    "        Q: torch.Tensor,\n",
    "        V: torch.Tensor,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "        output_hidden_states: bool = False,\n",
    "        return_dict: bool = True,\n",
    "    ) -> Union[tuple, BaseModelOutput]:\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        \n",
    "        # original_K = deepcopy(K)\n",
    "        # original_Q = deepcopy(Q)\n",
    "        # original_V = deepcopy(V)\n",
    "        \n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                \n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                layer_outputs = self._gradient_checkpointing_func(\n",
    "                    layer_module.__call__,\n",
    "                    K,\n",
    "                    Q,\n",
    "                    V,\n",
    "                    layer_head_mask,\n",
    "                    output_attentions,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(K, Q, V, layer_head_mask, output_attentions)\n",
    "\n",
    "            K = layer_outputs[0]\n",
    "            Q = layer_outputs[1]\n",
    "            V = layer_outputs[2]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "        \n",
    "        # output = self.last_layer(K, Q, V, layer_head_mask, output_attentions)\n",
    "        \n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n",
    "        \n",
    "        return (K,Q,V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-3.1615e-01, -1.3143e-01, -2.8139e-01,  ..., -3.5377e-01,\n",
       "           -5.3602e-01, -7.8833e-03],\n",
       "          [-3.1626e-01, -1.3176e-01, -2.8034e-01,  ..., -3.5475e-01,\n",
       "           -5.3714e-01, -7.4752e-03],\n",
       "          [-3.1657e-01, -1.3229e-01, -2.8067e-01,  ..., -3.5465e-01,\n",
       "           -5.3572e-01, -6.6319e-03],\n",
       "          ...,\n",
       "          [-3.1600e-01, -1.3196e-01, -2.8067e-01,  ..., -3.5446e-01,\n",
       "           -5.3615e-01, -7.3324e-03],\n",
       "          [-3.1570e-01, -1.3198e-01, -2.8106e-01,  ..., -3.5501e-01,\n",
       "           -5.3672e-01, -6.7696e-03],\n",
       "          [-3.1532e-01, -1.3145e-01, -2.8038e-01,  ..., -3.5453e-01,\n",
       "           -5.3662e-01, -7.7325e-03]],\n",
       " \n",
       "         [[-3.6240e-01, -1.3286e-01, -2.3638e-01,  ..., -3.5989e-01,\n",
       "           -4.9631e-01, -6.5904e-05],\n",
       "          [-3.6203e-01, -1.3460e-01, -2.3592e-01,  ..., -3.6099e-01,\n",
       "           -4.9695e-01,  6.0912e-04],\n",
       "          [-3.6224e-01, -1.3364e-01, -2.3632e-01,  ..., -3.6007e-01,\n",
       "           -4.9661e-01,  7.4923e-04],\n",
       "          ...,\n",
       "          [-3.6210e-01, -1.3406e-01, -2.3643e-01,  ..., -3.6084e-01,\n",
       "           -4.9700e-01,  5.2877e-04],\n",
       "          [-3.6283e-01, -1.3417e-01, -2.3592e-01,  ..., -3.6110e-01,\n",
       "           -4.9696e-01,  6.0963e-04],\n",
       "          [-3.6211e-01, -1.3451e-01, -2.3373e-01,  ..., -3.5983e-01,\n",
       "           -4.9663e-01, -2.0419e-04]]], grad_fn=<AddBackward0>),\n",
       " tensor([[ 0.1685, -0.0208,  0.1401,  0.0293],\n",
       "         [ 0.1587,  0.0191,  0.0936,  0.0086]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ViTIntegrated(nn.Module):\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.CNN_backbone = CNNBackBone(hidden_size = config.hidden_size)\n",
    "        self.ViT = ViTEncoder_modified(config)\n",
    "        # NOTE: the following code is designed for image size of 512*512\n",
    "        self.ViT_embedder_K = ViTEmbeddings_modified(config)\n",
    "        self.ViT_embedder_Q = ViTEmbeddings_modified(config)\n",
    "        self.ViT_embedder_V = ViTEmbeddings(config)\n",
    "        \n",
    "        # Classifier head\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels) if config.num_labels > 0 else nn.Identity()\n",
    "        \n",
    "    def forward(self, image):\n",
    "        if image.shape[-3:] != torch.Size([3,512,512]):\n",
    "            raise ValueError(f\"Input image dimension is not (3,512,512).\")\n",
    "        else:\n",
    "            K, Q = self.CNN_backbone(image)\n",
    "            K = self.ViT_embedder_K(K)\n",
    "            Q = self.ViT_embedder_Q(Q)\n",
    "            V = self.ViT_embedder_V(image)\n",
    "            if (K.shape != V.shape) or (K.shape != Q.shape):\n",
    "                raise ValueError(f\"Key, Quary, or Value dimension is not the same\")\n",
    "\n",
    "            ViT_outputs = self.ViT(\n",
    "                K,\n",
    "                Q,\n",
    "                V,\n",
    "            )\n",
    "\n",
    "            sequence_output = ViT_outputs[0] + ViT_outputs[1] + ViT_outputs[2]\n",
    "\n",
    "            logits = self.classifier(sequence_output[:, 0, :])\n",
    "            return sequence_output, logits\n",
    "            \n",
    "m = ViTIntegrated(ViTConfig(image_size =512,num_labels = 4,patch_size = 32)).to(\"cpu\")\n",
    "m(torch.randn((2,3,512,512)).to(\"cpu\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
