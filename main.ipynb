{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impoert Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import glob\n",
    "import math\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as TF\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import ViTConfig, ViTModel\n",
    "from PIL import Image\n",
    "from collections import OrderedDict \n",
    "from typing import Dict, List, Optional, Set, Tuple, Union\n",
    "from src.transformers.activations import ACT2FN\n",
    "from src.transformers.modeling_outputs import (\n",
    "    BaseModelOutput,\n",
    ")\n",
    "from copy import deepcopy\n",
    "from src.transformers.models.vit.modeling_vit import ViTEmbeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN BackBones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelHook(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch module to retrieve the output of specified layers in a model using forward hooks.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model from which the output is to be retrieved.\n",
    "        output_layers (list): A list of layer names for which the output needs to be captured.\n",
    "\n",
    "    Attributes:\n",
    "        output_layers (list): A list of layer names for which the output needs to be captured.\n",
    "        selected_out (OrderedDict): A dictionary to store the output of selected layers.\n",
    "        model (nn.Module): The model from which the output is retrieved.\n",
    "        fhooks (list): A list to hold the forward hooks registered for selected layers.\n",
    "\n",
    "    Methods:\n",
    "        forward_hook(layer_name): Method to create a forward hook for a specific layer.\n",
    "        forward(x): Forward method of the module.\n",
    "\n",
    "    Returns:\n",
    "        out (torch.Tensor): The output tensor from the model's forward pass.\n",
    "        selected_out (OrderedDict): A dictionary containing the output tensors of selected layers.\n",
    "\n",
    "    Example:\n",
    "        # Instantiate a ResNet model\n",
    "        resnet_model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "        # Define layers for which output needs to be captured\n",
    "        output_layers = ['conv1', 'layer1', 'layer2']\n",
    "\n",
    "        # Instantiate ModelHook module\n",
    "        model_hook = ModelHook(resnet_model, output_layers)\n",
    "\n",
    "        # Forward pass\n",
    "        inputs = torch.randn(1, 3, 224, 224)\n",
    "        out, selected_out = model_hook(inputs)\n",
    "\n",
    "        # Output of selected layers can be accessed from 'selected_out' dictionary\n",
    "        print(selected_out)\n",
    "    \"\"\"\n",
    "    def __init__(self,model, output_layers, *args):\n",
    "        super().__init__(*args)\n",
    "        self.output_layers = output_layers\n",
    "        # print(self.output_layers)\n",
    "        self.selected_out = OrderedDict()\n",
    "        #PRETRAINED MODEL\n",
    "        self.model = model\n",
    "        self.fhooks = []\n",
    "\n",
    "        for l in list(self.model._modules.keys()):\n",
    "            if l in self.output_layers:\n",
    "                self.fhooks.append(getattr(self.model,l).register_forward_hook(self.forward_hook(l)))\n",
    "    \n",
    "    def forward_hook(self,layer_name):\n",
    "        def hook(module, input, output):\n",
    "            self.selected_out[layer_name] = output\n",
    "        return hook\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out, self.selected_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBackBone(nn.Module):\n",
    "    def __init__(self, *args, hidden_size = 768, hidden_dropout_prob = 0.0,attention_probs_dropout_prob = 0.0):\n",
    "        super().__init__(*args)\n",
    "        # Load and impliment models\n",
    "        self.resnet50_module = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2).to(\"cuda\")\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=4, stride=4).to(\"cuda\")\n",
    "        self.middle_linear = nn.Linear(512, hidden_size).to(\"cuda\")\n",
    "        self.end_linear = nn.Linear(2048, hidden_size).to(\"cuda\")\n",
    "        \n",
    "        # Remove extra layers from CNN block (ResNetx) and add hook to it\n",
    "        layers_dict = {name: module for name,\n",
    "        module in zip(list(self.resnet50_module._modules.keys()),\n",
    "                             list(self.resnet50_module.children())[:-2])} #all layers except last two\n",
    "        self.resnet50_module = torch.nn.Sequential(OrderedDict(layers_dict))\n",
    "        self.CNN_block = ModelHook(self.resnet50_module, [\"layer2\",\"layer4\"])\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Send originad tgrough the Resnetx model to extract middle and end layer output\n",
    "        _, CNN_outputs = self.CNN_block(x)\n",
    "        \n",
    "        # Generate matrixe of size (-1,512,16,16) out of layer2 of ResNetx\n",
    "        CNN_middle_layer_out = self.avg_pool(CNN_outputs[\"layer2\"])\n",
    "        # Generate matrixe of size (-1,2048,16,16) out of layer4 of ResNetx\n",
    "        CNN_end_layer_out = CNN_outputs[\"layer4\"]\n",
    "        # print(CNN_middle_layer_out.shape)\n",
    "        # print(CNN_end_layer_out.shape)\n",
    "        \n",
    "        # Merge dimentions of heigth and width of matrixes into each other and swap dimentions to generate 256 vectors with the length of 512 and 2048\n",
    "        CNN_middle_layer_out = CNN_middle_layer_out.permute(0, 2, 3, 1).contiguous().view(CNN_middle_layer_out.shape[0], -1, 512)\n",
    "        CNN_end_layer_out = CNN_end_layer_out.permute(0, 2, 3, 1).contiguous().view(CNN_middle_layer_out.shape[0], -1, 2048)\n",
    "        # print(CNN_middle_layer_out.shape)\n",
    "        # print(CNN_end_layer_out.shape)\n",
    "        \n",
    "        # Send vectors throgh an MLP layer to make the generate vectors with length of 768\n",
    "        CNN_middle_layer_out = self.middle_linear(CNN_middle_layer_out)\n",
    "        CNN_end_layer_out = self.end_linear(CNN_end_layer_out)   \n",
    "        \n",
    "        print(CNN_middle_layer_out.shape)\n",
    "        print(CNN_end_layer_out.shape)\n",
    "        return CNN_end_layer_out, CNN_middle_layer_out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViTEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTEmbeddings_modified(nn.Module):\n",
    "\n",
    "    def __init__(self, config, num_patches=16*16):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
    "        \n",
    "        # NOTE: THE ORIGINAL PatchEmbeddings ELIMINATED\n",
    "        # self.patch_embeddings = PatchEmbeddings(\n",
    "        #     image_size=config.image_size,\n",
    "        #     patch_size=config.patch_size,\n",
    "        #     num_channels=config.num_channels,\n",
    "        #     embed_dim=config.hidden_size,\n",
    "        # )\n",
    "        # num_patches = self.patch_embeddings.num_patches\n",
    "        # self.patch_embeddings = CNNBackBone()\n",
    "        \n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches + 1, config.hidden_size))\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self, pixel_values):\n",
    "        print(\"modified embedding\")\n",
    "        batch_size = pixel_values.shape[0]\n",
    "        \n",
    "        # NOTE: THE ORIGINAL embedings ELIMINATED\n",
    "        # embeddings = self.patch_embeddings(pixel_values)\n",
    "        embeddings = pixel_values\n",
    "        \n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n",
    "        embeddings = embeddings + self.position_embeddings\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViTSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" this class does not modified\"\"\"\n",
    "class ViTSelfOutput(nn.Module):\n",
    "    \"\"\"\n",
    "    The residual connection is defined in ViTLayer instead of here (as is the case with other models), due to the\n",
    "    layernorm applied before each block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTSelfAttention_modified(nn.Module):\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # NOTE: i think we should modify the followint if\n",
    "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
    "            raise ValueError(\n",
    "                f\"The hidden size {config.hidden_size,} is not a multiple of the number of attention \"\n",
    "                f\"heads {config.num_attention_heads}.\"\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self, K,Q,V, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n",
    "    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n",
    "        mixed_query_layer = self.query(Q)\n",
    "\n",
    "        key_layer = self.transpose_for_scores(self.key(K))\n",
    "        value_layer = self.transpose_for_scores(self.value(V))\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "\n",
    "        return outputs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTSelfOutput(nn.Module):\n",
    "    '''this class does not get modified'''\n",
    "    \"\"\"\n",
    "    The residual connection is defined in ViTLayer instead of here (as is the case with other models), due to the\n",
    "    layernorm applied before each block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViTAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTAttention_modified(nn.Module):\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.attention = ViTSelfAttention_modified(config)\n",
    "        self.output = ViTSelfOutput(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        K: torch.Tensor,\n",
    "        Q: torch.Tensor,\n",
    "        V: torch.Tensor,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n",
    "        self_outputs = self.attention(K,Q,V, head_mask, output_attentions)\n",
    "        # NOTE: K, Q, and V can have seperate dense layers\n",
    "        attention_output = self.output(self_outputs[0], None)\n",
    "\n",
    "        #NOTE: if output_attentions is True modify the following code\n",
    "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViTLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'this class does not get modified'\n",
    "class ViTIntermediate(nn.Module):\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.intermediate_act_fn = config.hidden_act\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'this class does not get modified'\n",
    "class ViTOutput(nn.Module):\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        hidden_states = hidden_states #+ input_tensor\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTLayer_modified(nn.Module):\n",
    "    \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n",
    "\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "        self.seq_len_dim = 1\n",
    "        self.attention = ViTAttention_modified(config)\n",
    "        \n",
    "        self.intermediate_K = ViTIntermediate(config)\n",
    "        self.intermediate_Q = ViTIntermediate(config)\n",
    "        self.intermediate_V = ViTIntermediate(config)\n",
    "        \n",
    "        self.output_K = ViTOutput(config)\n",
    "        self.output_Q = ViTOutput(config)\n",
    "        self.output_V = ViTOutput(config)\n",
    "        \n",
    "        self.layernorm_before_k = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.layernorm_before_Q = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.layernorm_before_V = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        \n",
    "        self.layernorm_after_K = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.layernorm_after_Q = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.layernorm_after_V = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        K: torch.Tensor,\n",
    "        Q: torch.Tensor,\n",
    "        V: torch.Tensor,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n",
    "        tk = self.layernorm_before_k(K)\n",
    "        tq = self.layernorm_before_Q(Q)\n",
    "        tv = self.layernorm_before_V(V)\n",
    "        self_attention_outputs = self.attention(tk, tq, tv, head_mask, output_attentions=output_attentions)\n",
    "        attention_output = self_attention_outputs[0]\n",
    "        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "\n",
    "        # first residual connection\n",
    "        K_prime = attention_output + K\n",
    "        Q_prime = attention_output + Q\n",
    "        V_prime = attention_output + V\n",
    "        # in ViT, layernorm is also applied after self-attention\n",
    "        layer_output_K = self.layernorm_after_K(K_prime)\n",
    "        layer_output_Q = self.layernorm_after_Q(Q_prime)\n",
    "        layer_output_V = self.layernorm_after_V(V_prime)\n",
    "        \n",
    "        layer_output_K = self.intermediate_K(layer_output_K)\n",
    "        layer_output_Q = self.intermediate_Q(layer_output_Q)\n",
    "        layer_output_V = self.intermediate_V(layer_output_V)\n",
    "        # second residual connection is done here\n",
    "        layer_output_K = self.output_K(layer_output_K, None)\n",
    "        layer_output_Q = self.output_Q(layer_output_Q, None)\n",
    "        layer_output_V = self.output_V(layer_output_V, None)\n",
    "        \n",
    "        # outputs = (layer_output,) + outputs\n",
    "\n",
    "        return (layer_output_K, layer_output_Q, layer_output_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTLayer_modified_last_layer(nn.Module):\n",
    "    \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n",
    "\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "        self.seq_len_dim = 1\n",
    "        self.attention = ViTAttention_modified(config)\n",
    "        self.intermediate = ViTIntermediate(config)\n",
    "        self.output = ViTOutput(config)\n",
    "        self.layernorm_before_k = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.layernorm_before_Q = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.layernorm_before_V = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.layernorm_after = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        K: torch.Tensor,\n",
    "        Q: torch.Tensor,\n",
    "        V: torch.Tensor,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n",
    "        tk = self.layernorm_before_k(K)\n",
    "        tq = self.layernorm_before_Q(Q)\n",
    "        tv = self.layernorm_before_V(V)\n",
    "        self_attention_outputs = self.attention(tk, tq, tv, head_mask, output_attentions=output_attentions)\n",
    "        attention_output = self_attention_outputs[0]\n",
    "        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "\n",
    "        # first residual connection\n",
    "        hidden_states = attention_output + K + Q + V\n",
    "       \n",
    "        # in ViT, layernorm is also applied after self-attention\n",
    "        layer_output = self.layernorm_after(hidden_states)\n",
    "        layer_output = self.intermediate(layer_output)\n",
    "\n",
    "        # second residual connection is done here\n",
    "        layer_output = self.output(layer_output, hidden_states)\n",
    "\n",
    "        # outputs = (layer_output,) + outputs\n",
    "\n",
    "        return layer_output\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViTEncoder  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTEncoder_modified(nn.Module):\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        \n",
    "        self.layer = nn.ModuleList([ViTLayer_modified(config) for _ in range(config.num_hidden_layers)])\n",
    "        # self.last_layer = ViTLayer_modified_last_layer(config)\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        K: torch.Tensor,\n",
    "        Q: torch.Tensor,\n",
    "        V: torch.Tensor,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "        output_hidden_states: bool = False,\n",
    "        return_dict: bool = True,\n",
    "    ) -> Union[tuple, BaseModelOutput]:\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        \n",
    "        # original_K = deepcopy(K)\n",
    "        # original_Q = deepcopy(Q)\n",
    "        # original_V = deepcopy(V)\n",
    "        \n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                \n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                layer_outputs = self._gradient_checkpointing_func(\n",
    "                    layer_module.__call__,\n",
    "                    K,\n",
    "                    Q,\n",
    "                    V,\n",
    "                    layer_head_mask,\n",
    "                    output_attentions,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(K, Q, V, layer_head_mask, output_attentions)\n",
    "\n",
    "            K = layer_outputs[0]\n",
    "            Q = layer_outputs[1]\n",
    "            V = layer_outputs[2]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "        \n",
    "        # output = self.last_layer(K, Q, V, layer_head_mask, output_attentions)\n",
    "        \n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n",
    "        \n",
    "        return (K,Q,V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=None, logits=tensor([[ 0.2012,  0.4949, -1.1042, -0.0777],\n",
       "        [ 0.0965,  0.0273, -1.0214, -0.0175]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.transformers.models.vit.modeling_vit import ViTForImageClassification \n",
    "from src.transformers.models.vit.configuration_vit import ViTConfig \n",
    "import torch\n",
    "m = ViTForImageClassification(ViTConfig(image_size = 512,num_labels = 4))\n",
    "m(torch.randn((2,3,512,512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 257, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ViTEmbeddings(ViTConfig(image_size = 512, num_labels = 4, patch_size = 32), use_mask_token=False)(torch.randn((2,3,512,512))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 256, 768])\n",
      "torch.Size([2, 256, 768])\n",
      "modified embedding\n",
      "modified embedding\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.3971, -0.2705,  0.0592,  ..., -0.0290,  0.1837, -0.3098],\n",
       "          [ 0.3959, -0.2710,  0.0603,  ..., -0.0286,  0.1843, -0.3108],\n",
       "          [ 0.3959, -0.2701,  0.0596,  ..., -0.0291,  0.1850, -0.3099],\n",
       "          ...,\n",
       "          [ 0.3963, -0.2705,  0.0598,  ..., -0.0295,  0.1840, -0.3104],\n",
       "          [ 0.3962, -0.2707,  0.0588,  ..., -0.0297,  0.1840, -0.3098],\n",
       "          [ 0.3964, -0.2704,  0.0598,  ..., -0.0295,  0.1845, -0.3111]],\n",
       " \n",
       "         [[ 0.3678, -0.2275,  0.0216,  ..., -0.0092,  0.1984, -0.2983],\n",
       "          [ 0.3665, -0.2271,  0.0225,  ..., -0.0089,  0.1984, -0.2985],\n",
       "          [ 0.3663, -0.2274,  0.0219,  ..., -0.0092,  0.1986, -0.2976],\n",
       "          ...,\n",
       "          [ 0.3670, -0.2270,  0.0227,  ..., -0.0104,  0.1997, -0.2984],\n",
       "          [ 0.3662, -0.2268,  0.0217,  ..., -0.0084,  0.1986, -0.2982],\n",
       "          [ 0.3667, -0.2278,  0.0220,  ..., -0.0095,  0.1984, -0.2993]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[-0.1888, -0.1245,  0.0363,  0.0792],\n",
       "         [-0.1909, -0.1243,  0.0457,  0.0400]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ViTIntegrated(nn.Module):\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.CNN_backbone = CNNBackBone(hidden_size = config.hidden_size)\n",
    "        self.ViT = ViTEncoder_modified(config)\n",
    "        # NOTE: the following code is designed for image size of 512*512\n",
    "        self.ViT_embedder_K = ViTEmbeddings_modified(config)\n",
    "        self.ViT_embedder_Q = ViTEmbeddings_modified(config)\n",
    "        self.ViT_embedder_V = ViTEmbeddings(config)\n",
    "        \n",
    "        # Classifier head\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels) if config.num_labels > 0 else nn.Identity()\n",
    "        \n",
    "    def forward(self, image):\n",
    "        if image.shape[-3:] != torch.Size([3,512,512]):\n",
    "            raise ValueError(f\"Input image dimension is not (3,512,512).\")\n",
    "        else:\n",
    "            K, Q = self.CNN_backbone(image)\n",
    "            K = self.ViT_embedder_K(K)\n",
    "            Q = self.ViT_embedder_Q(Q)\n",
    "            V = self.ViT_embedder_V(image)\n",
    "            if (K.shape != V.shape) or (K.shape != Q.shape):\n",
    "                raise ValueError(f\"Key, Quary, or Value dimension is not the same\")\n",
    "\n",
    "            ViT_outputs = self.ViT(\n",
    "                K,\n",
    "                Q,\n",
    "                V,\n",
    "            )\n",
    "\n",
    "            sequence_output = ViT_outputs[0] + ViT_outputs[1] + ViT_outputs[2]\n",
    "\n",
    "            logits = self.classifier(sequence_output[:, 0, :])\n",
    "            return sequence_output, logits\n",
    "            \n",
    "m = ViTIntegrated(ViTConfig(image_size =512,num_labels = 4,patch_size = 32)).to(\"cpu\")\n",
    "m(torch.randn((2,3,512,512)).to(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 768])\n",
      "torch.Size([1, 256, 768])\n",
      "torch.Size([1, 256, 768])\n",
      "modified embedding\n",
      "torch.Size([1, 257, 768])\n",
      "modified embedding\n",
      "torch.Size([1, 257, 768])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((1,3,512,512)).to(\"cuda\")\n",
    "configuration = ViTConfig(image_size =256)\n",
    "out1_last, out1_middle = CNNBackBone()(x)\n",
    "print(out1_last.shape)\n",
    "out2_last = ViTEmbeddings_modified(configuration).to(\"cuda\")(out1_last)\n",
    "print(out2_last.shape)\n",
    "out2_middle = ViTEmbeddings_modified(configuration).to(\"cuda\")(out1_middle)\n",
    "print(out2_middle.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTConfig, ViTModel\n",
    "configuration = ViTConfig(image_size=256)\n",
    "\n",
    "# Initializing a model (with random weights) from the vit-base-patch16-224 style configuration\n",
    "model = ViTModel(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ViTLayer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 57\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m [hidden_states, all_hidden_states, all_self_attentions] \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutput(\n\u001b[0;32m     53\u001b[0m             last_hidden_state\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m     54\u001b[0m             hidden_states\u001b[38;5;241m=\u001b[39mall_hidden_states,\n\u001b[0;32m     55\u001b[0m             attentions\u001b[38;5;241m=\u001b[39mall_self_attentions,\n\u001b[0;32m     56\u001b[0m         )\n\u001b[1;32m---> 57\u001b[0m \u001b[43mViTEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 5\u001b[0m, in \u001b[0;36mViTEncoder.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([ViTLayer(config) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)])\n",
      "Cell \u001b[1;32mIn[18], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\u001b[43mViTLayer\u001b[49m(config) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ViTLayer' is not defined"
     ]
    }
   ],
   "source": [
    "class ViTEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer = nn.ModuleList([ViTLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        head_mask=None,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        return_dict=True,\n",
    "    ):\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        \n",
    "        \n",
    "\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "\n",
    "            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs, output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(layer_module),\n",
    "                    hidden_states,\n",
    "                    layer_head_mask,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=hidden_states,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "        )\n",
    "ViTEncoder(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 16, 16]         590,592\n",
      "ViTPatchEmbeddings-2             [-1, 256, 768]               0\n",
      "           Dropout-3             [-1, 257, 768]               0\n",
      "================================================================\n",
      "Total params: 590,592\n",
      "Trainable params: 590,592\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 4.51\n",
      "Params size (MB): 2.25\n",
      "Estimated Total Size (MB): 7.51\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "configuration = ViTConfig(image_size =256)\n",
    "model = ViTModel(configuration)\n",
    "\n",
    "summary(model.embeddings.to(\"cuda\"), (3, 256, 256))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 6 is not equal to len(dims) = 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m257\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mamdaliof\\anaconda3\\envs\\ML\\lib\\site-packages\\torchsummary\\torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[0;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(register_hook)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# remove these hooks\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "File \u001b[1;32mc:\\Users\\mamdaliof\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\mamdaliof\\anaconda3\\envs\\ML\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:407\u001b[0m, in \u001b[0;36mViTEncoder.forward\u001b[1;34m(self, hidden_states, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    400\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    401\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    402\u001b[0m         hidden_states,\n\u001b[0;32m    403\u001b[0m         layer_head_mask,\n\u001b[0;32m    404\u001b[0m         output_attentions,\n\u001b[0;32m    405\u001b[0m     )\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 407\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    409\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\mamdaliof\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\modules\\module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1535\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1536\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1538\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1540\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   1541\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1542\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1543\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\mamdaliof\\anaconda3\\envs\\ML\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:352\u001b[0m, in \u001b[0;36mViTLayer.forward\u001b[1;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    348\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    349\u001b[0m     head_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    350\u001b[0m     output_attentions: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    351\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor], Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[1;32m--> 352\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayernorm_before\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# in ViT, layernorm is applied before self-attention\u001b[39;49;00m\n\u001b[0;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    357\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    358\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mamdaliof\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\modules\\module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1535\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1536\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1538\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1540\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   1541\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1542\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1543\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\mamdaliof\\anaconda3\\envs\\ML\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:294\u001b[0m, in \u001b[0;36mViTAttention.forward\u001b[1;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    290\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    291\u001b[0m     head_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    292\u001b[0m     output_attentions: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    293\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor], Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[1;32m--> 294\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    298\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mamdaliof\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\modules\\module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1535\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1536\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1538\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1540\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   1541\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1542\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1543\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\mamdaliof\\anaconda3\\envs\\ML\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:214\u001b[0m, in \u001b[0;36mViTSelfAttention.forward\u001b[1;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m, hidden_states, head_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, output_attentions: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    211\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor], Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[0;32m    212\u001b[0m     mixed_query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery(hidden_states)\n\u001b[1;32m--> 214\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose_for_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(hidden_states))\n\u001b[0;32m    216\u001b[0m     query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n",
      "File \u001b[1;32mc:\\Users\\mamdaliof\\anaconda3\\envs\\ML\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:207\u001b[0m, in \u001b[0;36mViTSelfAttention.transpose_for_scores\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    205\u001b[0m new_x_shape \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_attention_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_head_size)\n\u001b[0;32m    206\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(new_x_shape)\n\u001b[1;32m--> 207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 6 is not equal to len(dims) = 4"
     ]
    }
   ],
   "source": [
    "summary(model.encoder.to(\"cuda\"), (1,1, 257, 768))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 257, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# configuration = ViTConfig(image_size =256)\n",
    "# m = ViTEmbeddings(configuration).to(\"cuda\")\n",
    "configuration = ViTConfig(image_size =256)\n",
    "m = ViTModel(configuration).to(\"cuda\").embeddings\n",
    "x = torch.randn((1,3,256,256)).to(\"cuda\")\n",
    "m(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPooling(last_hidden_state=tensor([[[-1.5152, -0.5085,  0.4542,  ...,  1.2464,  0.6460,  0.1602],\n",
       "         [ 0.1007, -1.1896,  0.0315,  ...,  0.1817,  0.4911,  0.1821],\n",
       "         [ 0.8041, -0.6515, -0.8861,  ..., -0.6614,  0.4310, -1.9497],\n",
       "         ...,\n",
       "         [ 1.0748,  1.5913, -0.5543,  ..., -1.4002, -0.0362,  0.3171],\n",
       "         [ 0.3562, -0.4976, -1.1586,  ...,  0.5436, -0.1705,  0.6474],\n",
       "         [ 0.7896,  0.0963, -0.9664,  ...,  1.1486,  1.2106,  0.1662]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.2430,  0.0300,  0.3841, -0.5894, -0.7924, -0.3783, -0.6950, -0.4077,\n",
       "         -0.8477, -0.3615,  0.4518,  0.1785,  0.2740,  0.2388, -0.6324, -0.1731,\n",
       "          0.3724,  0.0395,  0.2062, -0.2514,  0.8096, -0.5194, -0.3910,  0.3860,\n",
       "          0.0463,  0.6350, -0.2890, -0.7141,  0.2612, -0.4473,  0.1869,  0.3911,\n",
       "          0.5797, -0.6908,  0.3550,  0.0498, -0.6859, -0.3283, -0.4602, -0.3890,\n",
       "          0.0532, -0.7016,  0.1627, -0.3339, -0.4649, -0.3107, -0.4392, -0.5071,\n",
       "          0.0951,  0.7739, -0.6619,  0.7563, -0.2382,  0.5529, -0.1540,  0.3290,\n",
       "         -0.1143, -0.5992,  0.2993, -0.0519, -0.5721, -0.2593,  0.1675, -0.0054,\n",
       "          0.0776, -0.7684,  0.4609, -0.1632, -0.1706, -0.6103,  0.2689,  0.0621,\n",
       "          0.1192,  0.6907, -0.3038, -0.2447, -0.8983, -0.7326, -0.2370, -0.6030,\n",
       "          0.6162, -0.1866, -0.8495, -0.1299,  0.6378,  0.1614, -0.5059,  0.0951,\n",
       "         -0.3975, -0.2459,  0.6972,  0.5835,  0.4183,  0.3105,  0.8011,  0.3579,\n",
       "         -0.3144, -0.1434, -0.1924, -0.5953, -0.7307,  0.1427,  0.4054, -0.6166,\n",
       "         -0.1464,  0.8447,  0.7037, -0.3276, -0.3395, -0.7937,  0.5836, -0.0543,\n",
       "          0.1605,  0.2042, -0.3683, -0.3336,  0.5846,  0.4642,  0.0663,  0.1412,\n",
       "          0.6358, -0.7991, -0.4619,  0.4911,  0.7875,  0.4537, -0.6925, -0.5788,\n",
       "         -0.5779, -0.4870, -0.0514,  0.4763,  0.1565, -0.8123,  0.4597, -0.0863,\n",
       "          0.1959, -0.2589,  0.0414, -0.5079,  0.4981,  0.6999,  0.1465,  0.0081,\n",
       "         -0.4248,  0.3926, -0.4385, -0.8285, -0.5547, -0.5714, -0.6221,  0.3124,\n",
       "         -0.1440,  0.2684, -0.4105,  0.0547,  0.3779, -0.5518,  0.6302, -0.0195,\n",
       "          0.1423,  0.5724, -0.5508, -0.4542,  0.0484,  0.0036,  0.1241, -0.1022,\n",
       "          0.1928,  0.5545,  0.8312, -0.9321,  0.2855,  0.3616,  0.4374,  0.1677,\n",
       "         -0.5341, -0.4813,  0.5263, -0.3263, -0.2073, -0.3274, -0.2044, -0.1511,\n",
       "          0.1929,  0.1145, -0.2831,  0.1992,  0.1174, -0.1489,  0.5193, -0.1602,\n",
       "         -0.6275, -0.3000, -0.6796,  0.8827, -0.0203, -0.3029,  0.2695,  0.0450,\n",
       "         -0.3941, -0.2749,  0.9329, -0.0824, -0.7520, -0.5397,  0.1172, -0.5776,\n",
       "          0.5806,  0.7439, -0.5665,  0.2941,  0.0536, -0.1921, -0.7439,  0.1880,\n",
       "         -0.4841,  0.7960,  0.7158,  0.2905,  0.6694,  0.5965, -0.4525,  0.2955,\n",
       "         -0.6903, -0.0420, -0.5492, -0.3492, -0.0415,  0.2105,  0.3642,  0.1598,\n",
       "          0.2464, -0.0584,  0.3107,  0.4539,  0.4809, -0.1278,  0.2073, -0.1370,\n",
       "          0.6569, -0.2548,  0.5543, -0.1077, -0.8143, -0.1249,  0.2761,  0.5848,\n",
       "         -0.6247,  0.3598,  0.4950,  0.8936, -0.3048, -0.3714,  0.4882,  0.1799,\n",
       "         -0.2609, -0.2051, -0.2478, -0.4654, -0.2089, -0.8371, -0.2094,  0.2058,\n",
       "         -0.2513, -0.7777,  0.3440, -0.4374,  0.2027,  0.5837, -0.0589, -0.8041,\n",
       "         -0.0299, -0.0867, -0.1011, -0.7251,  0.6584,  0.2914,  0.0098,  0.6347,\n",
       "         -0.5924, -0.2950, -0.0988,  0.4882, -0.6585,  0.5343,  0.1963,  0.1497,\n",
       "         -0.4047, -0.9072, -0.4014,  0.4530, -0.5417, -0.0545, -0.1678, -0.5282,\n",
       "          0.4833,  0.1472,  0.8878,  0.1076, -0.4295, -0.3918,  0.1315, -0.3316,\n",
       "          0.3529, -0.3080, -0.5929, -0.7124,  0.2879,  0.3548, -0.3257,  0.3140,\n",
       "         -0.4515,  0.3733, -0.0473, -0.0845,  0.1901,  0.5231,  0.4432, -0.9181,\n",
       "          0.2313, -0.0739, -0.0890, -0.0359,  0.7582, -0.2860, -0.1266,  0.1777,\n",
       "         -0.6642, -0.2150,  0.7115, -0.0836, -0.1517, -0.0244, -0.0532,  0.4958,\n",
       "         -0.4207,  0.2048, -0.8147,  0.6385, -0.0473, -0.6113, -0.1406,  0.7017,\n",
       "          0.1700, -0.4762, -0.3902,  0.7943,  0.6185,  0.5859,  0.1392,  0.3253,\n",
       "          0.9005,  0.0464, -0.6901,  0.6601,  0.1675,  0.4150, -0.7192,  0.0860,\n",
       "         -0.3918, -0.2828, -0.3150,  0.1052, -0.1638, -0.0932,  0.4710, -0.8168,\n",
       "          0.3830,  0.6469, -0.3184, -0.4642,  0.5572,  0.7772,  0.4781,  0.8852,\n",
       "         -0.2498,  0.8019,  0.7241, -0.3817,  0.5794,  0.0536,  0.7325,  0.8341,\n",
       "         -0.2322,  0.2222, -0.7014,  0.3630, -0.2657, -0.3781,  0.4671, -0.1422,\n",
       "          0.5770,  0.1532, -0.1829, -0.1015, -0.3656, -0.1793, -0.6739,  0.0338,\n",
       "          0.3076,  0.8660, -0.0602, -0.2206,  0.1122,  0.1259,  0.1553, -0.5412,\n",
       "          0.7940, -0.0387,  0.7278, -0.4681,  0.1784,  0.1788, -0.2271, -0.7925,\n",
       "          0.0383,  0.4536,  0.2594, -0.7026,  0.4108, -0.1495, -0.0780,  0.2036,\n",
       "          0.1833, -0.6934,  0.3466, -0.4508, -0.8095,  0.5293, -0.5184,  0.6982,\n",
       "         -0.7637, -0.0553,  0.4579, -0.0356,  0.8672,  0.4847,  0.7162,  0.0828,\n",
       "          0.2586, -0.2795,  0.7207, -0.0720,  0.4054,  0.0449,  0.5568, -0.4804,\n",
       "         -0.0780,  0.5276, -0.4912, -0.3542, -0.2774, -0.5806, -0.0126, -0.2787,\n",
       "          0.2787,  0.5376,  0.5889,  0.1381, -0.1220,  0.3162,  0.5350, -0.2153,\n",
       "          0.6604, -0.5721, -0.4111,  0.7284,  0.7197,  0.5262, -0.0097, -0.1693,\n",
       "         -0.6560, -0.7762,  0.8088,  0.9233, -0.8504, -0.8066, -0.4678, -0.2182,\n",
       "          0.5325, -0.8014, -0.1284, -0.6206,  0.1460,  0.5930,  0.7415, -0.3767,\n",
       "          0.0497,  0.6073, -0.2551,  0.2246, -0.1809, -0.0524,  0.5926,  0.3156,\n",
       "         -0.5463,  0.3441,  0.3496,  0.0974,  0.1544,  0.3871, -0.0796,  0.8924,\n",
       "         -0.5039, -0.4623, -0.5457,  0.7650, -0.1475,  0.3029,  0.1595,  0.2566,\n",
       "          0.6311,  0.7704, -0.4196,  0.9428,  0.0980,  0.5707, -0.0881,  0.4538,\n",
       "         -0.6093,  0.4164, -0.1936, -0.3406, -0.4941,  0.6093,  0.0315,  0.2170,\n",
       "         -0.2615,  0.6650, -0.7968,  0.3580, -0.0682, -0.1309,  0.6709,  0.3481,\n",
       "          0.2233, -0.3263,  0.2614, -0.1846, -0.2168,  0.0535, -0.1318,  0.0947,\n",
       "         -0.7522,  0.2826, -0.3091,  0.6637,  0.4971, -0.2752,  0.1056,  0.0083,\n",
       "          0.6992, -0.2820, -0.1465, -0.4733, -0.0584, -0.4530,  0.6221,  0.5472,\n",
       "         -0.0062, -0.6189, -0.0574,  0.5153,  0.6402,  0.2939, -0.4518, -0.2918,\n",
       "          0.6195, -0.1361,  0.6142, -0.3338,  0.3936, -0.5421,  0.0894,  0.1731,\n",
       "          0.4524, -0.0217, -0.9094,  0.5522, -0.4516, -0.3315,  0.3862,  0.0679,\n",
       "          0.0826, -0.4336,  0.4207,  0.4568, -0.1201, -0.4129,  0.2938,  0.3938,\n",
       "          0.1361, -0.5441,  0.1671, -0.1967, -0.2076, -0.2016,  0.6682, -0.0076,\n",
       "         -0.2023, -0.3609, -0.0632,  0.2636, -0.0624, -0.1582, -0.6109, -0.5297,\n",
       "         -0.2127, -0.8019, -0.4311,  0.7976,  0.2018,  0.1939,  0.0131, -0.1924,\n",
       "         -0.1979, -0.2935,  0.6140, -0.3199,  0.7287,  0.7969,  0.1103,  0.5951,\n",
       "         -0.5503,  0.6831, -0.3552, -0.0367, -0.4151, -0.6580,  0.0200, -0.8094,\n",
       "         -0.1265, -0.5656,  0.4089, -0.0362, -0.0889,  0.4951, -0.4196,  0.9782,\n",
       "         -0.0772, -0.0378, -0.7356,  0.4319, -0.4965, -0.8734, -0.7179,  0.5082,\n",
       "          0.6637,  0.4560, -0.2445,  0.0625,  0.5527,  0.3952,  0.5451, -0.0508,\n",
       "          0.0178, -0.0718, -0.3304, -0.6116,  0.2240,  0.7806, -0.1762, -0.7873,\n",
       "         -0.6139, -0.7759,  0.0292,  0.3105, -0.7239, -0.5589, -0.8387, -0.3345,\n",
       "          0.3042,  0.5840, -0.5869, -0.4425,  0.4652, -0.4261, -0.2048,  0.2712,\n",
       "         -0.4169, -0.1135,  0.5574, -0.2209,  0.3545, -0.4806,  0.1834,  0.0757,\n",
       "          0.4081,  0.5887, -0.2751, -0.4500, -0.4789, -0.4555,  0.7911,  0.4563,\n",
       "         -0.4276,  0.8799,  0.0156,  0.8391, -0.6496, -0.2816,  0.4244, -0.4263,\n",
       "         -0.1547, -0.2638, -0.0732,  0.3475, -0.1579, -0.4043,  0.3720, -0.1090,\n",
       "          0.0333,  0.7319, -0.4004,  0.0295, -0.7861, -0.6121,  0.7975,  0.1250,\n",
       "         -0.2217,  0.7074,  0.5980, -0.0144,  0.2325,  0.3160,  0.1748, -0.3241,\n",
       "         -0.8421, -0.3163,  0.5672,  0.5750, -0.0813,  0.9184,  0.3675,  0.3959,\n",
       "         -0.4174,  0.6843, -0.3258, -0.6899,  0.1055,  0.1441,  0.3751, -0.5806,\n",
       "         -0.3183,  0.0228,  0.9070, -0.3261, -0.1502,  0.1253,  0.1519,  0.1944,\n",
       "         -0.3885,  0.3916, -0.4244,  0.4788,  0.3196, -0.5414,  0.2808,  0.3560,\n",
       "          0.4527, -0.2910, -0.5438, -0.0401,  0.1336, -0.2018, -0.7440,  0.8447]],\n",
       "       device='cuda:0', grad_fn=<TanhBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.transformers.models.vit.modeling_vit import ViTEncoder, ViTAttention, ViTSelfAttention, ViTModel\n",
    "from src.transformers.models.vit.configuration_vit import ViTConfig\n",
    "\n",
    "import torch\n",
    "configuration = ViTConfig(image_size =256)\n",
    "m = ViTModel(configuration).to(\"cuda\")\n",
    "# m = ViTSelfAttention(configuration).to(\"cuda\")\n",
    "# x = torch.randn((1, 257, 768)).to(\"cuda\")\n",
    "x = torch.randn((1, 3, 256, 256)).to(\"cuda\")\n",
    "m(x)\n",
    "# m = ViTEncoder(configuration).to(\"cuda\")\n",
    "# m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
